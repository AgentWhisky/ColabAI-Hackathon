<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[EAGER: RI: Enabling Natural Language and Decision Making Capabilities of Robotic Guide Dogs for People with Visual Impairment]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2024</AwardEffectiveDate>
<AwardExpirationDate>06/30/2025</AwardExpirationDate>
<AwardTotalIntnAmount>125000.00</AwardTotalIntnAmount>
<AwardAmount>125000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Erion Plaku</SignBlockName>
<PO_EMAI>eplaku@nsf.gov</PO_EMAI>
<PO_PHON>7032924426</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Guide dogs are trained to serve people who have severely limited or no vision by helping them avoid obstacles and other dangers while walking. Research has shown that guide dogs improve the lives of visually impaired people by increasing independence, confidence, companionship, and mobility. However, in the US, only about two percent of the visually impaired people work with guide dogs. Example reasons include high training costs, low graduation rates and the everyday care needed by the dogs. Quadruped robots are increasingly drawing attention from the robotics community because they are more versatile than wheeled robots and can traverse many different terrains. The goal of this project is to provide a low-cost, user-friendly solution for guiding visually impaired people, significantly improving their quality of life. &lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Little research on quadruped robotics has been done to look into the interaction and collaboration aspects of human-quadruped systems. The primary focus of this project is on the human-robot-environment, three-way interaction, and the goal is to enable the following novel capabilities for robotic guide dogs. The first capability is dialog and scene verbalization, which will enable the robotic guide dogs to interact with the visually impaired using natural language. The second capability is intelligent disobedience. For robotic guide dogs, intelligent disobedience requires at least the capabilities of visual scene analysis and commonsense reasoning. For learning and evaluation purposes, this project will produce a dataset that includes 360-degree images and safety labels in different directions. Further, the investigator and team will leverage the state-of-the-art computer vision techniques for making disobedience decisions. The first research aim on the language capabilities will improve the transparency of disobedience decisions in the second research aim. This project will generate the datasets and prototypes towards complete systems and comprehensive evaluations of robotic guide dogs. Guide dog trainers and people with visual impairments will be included and play a key role in this project.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/09/2024</MinAmdLetterDate>
<MaxAmdLetterDate>07/09/2024</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2428998</AwardID>
<Investigator>
<FirstName>Shiqi</FirstName>
<LastName>Zhang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shiqi Zhang</PI_FULL_NAME>
<EmailAddress><![CDATA[zhangs@binghamton.edu]]></EmailAddress>
<NSF_ID>000753721</NSF_ID>
<StartDate>07/09/2024</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[SUNY at Binghamton]]></Name>
<CityName>BINGHAMTON</CityName>
<ZipCode>139024400</ZipCode>
<PhoneNumber>6077776136</PhoneNumber>
<StreetAddress><![CDATA[4400 VESTAL PKWY E]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>19</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY19</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>NQMVAAQUFU53</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM>GMZUKXFDJMA9</ORG_PRNT_UEI_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SUNY at Binghamton]]></Name>
<CityName>BINGHAMTON</CityName>
<StateCode>NY</StateCode>
<ZipCode>139024400</ZipCode>
<StreetAddress><![CDATA[4400 VESTAL PKWY E]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>19</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY19</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>749500</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Fund>
<Code>01002425DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2024~125000</FUND_OBLG>
</Award>
</rootTag>
