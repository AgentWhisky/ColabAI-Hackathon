<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Collaborative Research: CIF: Medium: Towards optimal scheduling for parallelizable machine learning training workloads]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2024</AwardEffectiveDate>
<AwardExpirationDate>05/31/2028</AwardExpirationDate>
<AwardTotalIntnAmount>363266.00</AwardTotalIntnAmount>
<AwardAmount>174200</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Alfred Hero</SignBlockName>
<PO_EMAI>ahero@nsf.gov</PO_EMAI>
<PO_PHON>7032920000</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In the past fifteen years, there has been explosive growth in the number of applications seeking to leverage Machine Learning (ML) models. Before an ML model can be deployed, the model must be “trained” by repeatedly processing examples. Each example helps the model to make progressively more accurate predictions, "learning" how to solve the problem at hand. Unfortunately, this training step requires a large amount of expensive, highly-specialized hardware and can take several hours to complete. Given limited hardware resources, it is not obvious how to allocate (share) these resources across a stream of ML training jobs. The goal of this project is to develop new resource allocation policies that allow us to produce highly-accurate ML models, quickly, and with limited resources. The main challenge in this work is that ML training jobs present a number of unique characteristics compared to other computing workloads. ML training jobs are highly parallelizable, meaning a single training job might run across multiple servers. Each job also has a large number of configuration options that affect how fast the model learns a particular problem. This project uses mathematical modeling to develop new allocation policies specifically designed for ML training jobs. This research will be accompanied by the development of new courses and mentorship programs designed to recruit students from underrepresented backgrounds into research on the modeling of computer systems.&lt;br/&gt;&lt;br/&gt;Machine Learning (ML) models are increasingly being deployed across a wide variety of applications. The growth in ML models has been accompanied by the development of specialized hardware accelerators that help reduce the training time for ML models. However, there has not been a similar degree of specialization in the scheduling algorithms used to train  ML models on clusters of specialized hardware. The question of how to best schedule ML training jobs is non-trivial. ML training jobs present many unique characteristics compared to other computing workloads. First, ML training jobs vary in their degree of parallelizability (ability to scale out across servers), with some jobs being highly elastic and others being inelastic in their scalability. It is not clear how to allocate (share) hardware resources among jobs with such different characteristics. To make things more complicated, each training job is parameterized by a set of tuning parameters called hyperparameters. The parallelizability of an ML training job varies over time as the job hyperparameters change. Hence, systems which either rely on static resource reservations or existing heuristic scheduling policies are poorly suited for ML training jobs. Finally, the inherent work associated with an ML job is not a fixed quantity. Instead, ML jobs are generally trained until the model meets a desired level of accuracy. Hence, the existing scheduling theory that favors “short” jobs does not apply in this case. This proposal develops a new theoretic framework for modeling the dynamics of ML jobs running in a shared cluster. This framework will be used to develop scheduling and allocation policies that are specialized to ML training jobs, and to prove performance guarantees on these policies.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/03/2024</MinAmdLetterDate>
<MaxAmdLetterDate>06/03/2024</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2403195</AwardID>
<Investigator>
<FirstName>Benjamin</FirstName>
<LastName>Berg</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Benjamin Berg</PI_FULL_NAME>
<EmailAddress><![CDATA[ben@cs.unc.edu]]></EmailAddress>
<NSF_ID>000919209</NSF_ID>
<StartDate>06/03/2024</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[University of North Carolina at Chapel Hill]]></Name>
<CityName>CHAPEL HILL</CityName>
<ZipCode>275995023</ZipCode>
<PhoneNumber>9199663411</PhoneNumber>
<StreetAddress><![CDATA[104 AIRPORT DR STE 2200]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>D3LHU66KBLD5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM>D3LHU66KBLD5</ORG_PRNT_UEI_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of North Carolina at Chapel Hill]]></Name>
<CityName>CHAPEL HILL</CityName>
<StateCode>NC</StateCode>
<ZipCode>275995023</ZipCode>
<StreetAddress><![CDATA[104 AIRPORT DR STE 2200]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>779700</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Fund>
<Code>01002425DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2024~174200</FUND_OBLG>
</Award>
</rootTag>
