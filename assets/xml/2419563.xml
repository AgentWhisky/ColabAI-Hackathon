<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Collaborative Research: Safe Reinforcement Learning Guaranteed by Bayesian Distributionally Robust Optimization and Online Change Point Detection]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2024</AwardEffectiveDate>
<AwardExpirationDate>08/31/2027</AwardExpirationDate>
<AwardTotalIntnAmount>193000.00</AwardTotalIntnAmount>
<AwardAmount>193000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07010000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>ECCS</Abbreviation>
<LongName>Div Of Electrical, Commun &amp; Cyber Sys</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Anthony Kuh</SignBlockName>
<PO_EMAI>akuh@nsf.gov</PO_EMAI>
<PO_PHON>7032924714</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Safety is a crucial requirement for systems employing reinforcement learning in domains such as&lt;br/&gt;robotics, autonomous driving, and power systems. In this project we consider safety as the avoidance of known unsafe states and prevention of unknown unsafe behaviors. To achieve this safety goal, we propose a suite of model-based reinforcement learning approaches that span training, deployment, improvement, and evaluation. The project consists of the following research thrusts: 1) Training policies that are robust to distribution shift via distributionally robust approaches; 2) Continual policy improvement via Bayesian risk-averse learning; 3) Adapting policies to non-stationarity via online change detection; and 4) Rigorous simulation via space-filling experiment design to gain understandings of a given policy in various environment settings.&lt;br/&gt; &lt;br/&gt;If successful, the proposed research will make significant contributions to the existing literature on safe reinforcement learning (RL) by developing new theories and methodologies. In particular, the proposed research has the following innovations: 1) formulation of safety measures as general objectives beyond the standard cumulative form and development of solution approaches for this general formulation; 2) consideration of both intrinsic uncertainty and model uncertainty to ensure that the resulting policy performs well and satisfies a specified risk level in the real environment; 3) bridging the gap between Bayesian RL and safe RL for continually improving models and policies while maintaining the safety of the deployed policy; 4) near-optimal policy learning algorithms that adapt to piecewise non-stationary environments; and 5) rigorous simulation approach for policy evaluation to identify unexpected unsafe behaviors before they actually happen. Because of the generality of the proposed approaches, the resulting techniques will have broad applicability in various domains that utilize reinforcement learning and require safety considerations. This research integrates well with the courses that the PIs have developed and teach. The PIs are committed to promoting diversity, equity, and inclusion within their research communities by actively engaging women and minorities in research and academia careers, outreaching to K-12 students, and fostering greater participation of researchers from underrepresented groups.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/22/2024</MinAmdLetterDate>
<MaxAmdLetterDate>07/22/2024</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2419563</AwardID>
<Investigator>
<FirstName>Mengdi</FirstName>
<LastName>Wang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Mengdi Wang</PI_FULL_NAME>
<EmailAddress><![CDATA[mengdiw@princeton.edu]]></EmailAddress>
<NSF_ID>000712266</NSF_ID>
<StartDate>07/22/2024</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>PRINCETON</CityName>
<ZipCode>085442001</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress><![CDATA[1 NASSAU HALL]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>NJ1YPQXQG7U5</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>THE TRUSTEES OF PRINCETON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>PRINCETON</CityName>
<StateCode>NJ</StateCode>
<ZipCode>085442001</ZipCode>
<StreetAddress><![CDATA[1 NASSAU HALL]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>760700</Code>
<Text>EPCN-Energy-Power-Ctrl-Netwrks</Text>
</ProgramElement>
<ProgramReference>
<Code>8888</Code>
<Text>LEARNING &amp; INTELLIGENT SYSTEMS</Text>
</ProgramReference>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Fund>
<Code>01002425DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2024~193000</FUND_OBLG>
</Award>
</rootTag>
