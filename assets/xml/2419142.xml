<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[I-Corps: Translation Potential of Bidirectional Neural Communication for Extended Reality Technologies]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2024</AwardEffectiveDate>
<AwardExpirationDate>03/31/2025</AwardExpirationDate>
<AwardTotalIntnAmount>50000.00</AwardTotalIntnAmount>
<AwardAmount>50000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>15030000</Code>
<Directorate>
<Abbreviation>TIP</Abbreviation>
<LongName>Dir for Tech, Innovation, &amp; Partnerships</LongName>
</Directorate>
<Division>
<Abbreviation>TI</Abbreviation>
<LongName>Translational Impacts</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Molly Wasko</SignBlockName>
<PO_EMAI>mwasko@nsf.gov</PO_EMAI>
<PO_PHON>7032924749</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact of this I-Corps project is the development of next-generation extended reality technologies with integrated wearable biomedical sensors. The development of wearable technology for Augmented Reality/Virtual Reality (AR/VR) headsets has the potential to change how people use and embrace these technologies. By creating a controller platform that relies on user feedback, people can interact with devices that are more user-friendly. Additionally, the ability of these devices to detect materials can allow users to interact with virtual objects using natural hand gestures. By improving the understanding of and response to human actions, these devices will make the AR/VR experience more immersive and enjoyable. Overall, this advancement brings the users closer to seamlessly blending the real and virtual worlds, potentially making these technologies more accessible and beneficial.&lt;br/&gt;&lt;br/&gt;This I-Corps project utilizes experiential learning coupled with a first-hand investigation of the industry ecosystem to assess the translation potential of the technology. The solution is based on the development of a cost-effective, waterproof sensor made from carbon for Augmented Reality/Virtual Reality (AR/VR) headsets. This sensor will gather data on eye movements (electrooculography - EOG), muscle activity (electromyography - EMG), and brainwaves (electroencephalography - EEG), allowing for the precise prediction of human emotions and responses. The new wearable technology for AR/VR headsets introduces a versatile sensing platform to enable two-way neural communication between users and devices. Its soft, flexible design minimizes motion for accurate data collection, while carbon-based electrodes help maintain a high signal-to-noise ratio. Simultaneously, a piezoelectric sensor detects hand gestures and materials, dynamically enhancing user interaction. This advancement integrates seamlessly with headsets, offering a cost-effective, scalable, and waterproof solution. Drawing from neuroscience, materials science, and artificial intelligence, this interdisciplinary approach expands the possibilities of wearable technology, promising an enhanced user experiences in AR/VR environments.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>04/01/2024</MinAmdLetterDate>
<MaxAmdLetterDate>04/01/2024</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.084</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2419142</AwardID>
<Investigator>
<FirstName>Huanyu</FirstName>
<LastName>Cheng</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Huanyu Cheng</PI_FULL_NAME>
<EmailAddress><![CDATA[huc24@psu.edu]]></EmailAddress>
<NSF_ID>000702097</NSF_ID>
<StartDate>04/01/2024</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name><![CDATA[Pennsylvania State Univ University Park]]></Name>
<CityName>UNIVERSITY PARK</CityName>
<ZipCode>168021503</ZipCode>
<PhoneNumber>8148651372</PhoneNumber>
<StreetAddress><![CDATA[201 OLD MAIN]]></StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>15</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA15</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>NPM2J7MSCF61</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>THE PENNSYLVANIA STATE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Pennsylvania State Univ University Park]]></Name>
<CityName>UNIVERSITY PARK</CityName>
<StateCode>PA</StateCode>
<ZipCode>168021503</ZipCode>
<StreetAddress><![CDATA[201 OLD MAIN]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>15</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA15</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>802300</Code>
<Text>I-Corps</Text>
</ProgramElement>
<ProgramReference>
<Code>079E</Code>
<Text>VISUALIZATION &amp; VIRTUAL DESIGN</Text>
</ProgramReference>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Fund>
<Code>01002425DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2024~50000</FUND_OBLG>
</Award>
</rootTag>
