<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle><![CDATA[Collaborative Research: SHF: Small: Efficient and Scalable Privacy-Preserving Neural Network Inference based on Ciphertext-Ciphertext Fully Homomorphic Encryption]]></AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2024</AwardEffectiveDate>
<AwardExpirationDate>03/31/2026</AwardExpirationDate>
<AwardTotalIntnAmount>275000.00</AwardTotalIntnAmount>
<AwardAmount>240062</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Danella Zhao</SignBlockName>
<PO_EMAI>dzhao@nsf.gov</PO_EMAI>
<PO_PHON>7032924434</PO_PHON>
</ProgramOfficer>
<AbstractNarration><![CDATA[Along with the evolution of artificial intelligence, privacy-preserving machine learning has emerged as an important and promising technique for protecting user-data privacy in cloud applications. Among the existing approaches, fully homomorphic encryption (FHE) based methods allow machine learning algorithms to be computed on encrypted data, while no original data information is leaked. This project addresses ciphertext-ciphertext FHE that preserves the privacy of both the user and model providers. This project aims to improve the hardware efficiency of ciphertext-ciphertext FHE-based neural network inference by orders of magnitude through algorithm-hardware co-optimization. This project yields a novel framework for ensuring the root of trust in cloud computing and cryptosystems to meet the future needs of both commercial products and national defense.&lt;br/&gt;&lt;br/&gt;This project develops efficient and scalable hardware architectures for privacy-preserving neural network inference based on ciphertext-ciphertext FHE. This project leverages scheme switching - using arithmetic-based schemes for linear functions and Boolean logic-based schemes for non-linear functions - to accelerate the neural network computations. Research thrusts include: a) Designing efficient fundamental hardware building blocks with high scalability over word-length of modulus and degree of polynomial for privacy-preserving neural network, i.e., polynomial multipliers, by employing novel reconfigurable and pipelining framework and exploiting special primes to perform fast modular reduction; b) Further improving the efficiency of polynomial multiplier designs by utilizing a divide and conquer strategy based on a novel parallel filter technique; c) Developing a reconfigurable and neural network friendly FHE architecture using scheme switching; and d) Designing an efficient accelerator of privacy-preserving neural network inference with ciphertext-ciphertext operations via scheme switching that protects the privacy of both the user and model.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.]]></AbstractNarration>
<MinAmdLetterDate>02/21/2024</MinAmdLetterDate>
<MaxAmdLetterDate>02/21/2024</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2412357</AwardID>
<Investigator>
<FirstName>Yingjie</FirstName>
<LastName>Lao</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yingjie Lao</PI_FULL_NAME>
<EmailAddress><![CDATA[yingjie.lao@tufts.edu]]></EmailAddress>
<NSF_ID>000729841</NSF_ID>
<StartDate>02/21/2024</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Tufts University</Name>
<CityName>SOMERVILLE</CityName>
<ZipCode>021442401</ZipCode>
<PhoneNumber>6176273696</PhoneNumber>
<StreetAddress>169 HOLLAND ST</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_UEI_NUM>WL9FLBRVPJJ7</ORG_UEI_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF TUFTS COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_UEI_NUM>WL9FLBRVPJJ7</ORG_PRNT_UEI_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Tufts University]]></Name>
<CityName>SOMERVILLE</CityName>
<StateCode>MA</StateCode>
<ZipCode>021442401</ZipCode>
<StreetAddress><![CDATA[169 HOLLAND ST]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramReference>
<Appropriation>
<Code/>
<Name/>
<APP_SYMB_ID/>
</Appropriation>
<Fund>
<Code>01002324DB</Code>
<Name><![CDATA[NSF RESEARCH & RELATED ACTIVIT]]></Name>
<FUND_SYMB_ID>040100</FUND_SYMB_ID>
</Fund>
<FUND_OBLG>2023~240062</FUND_OBLG>
</Award>
</rootTag>
